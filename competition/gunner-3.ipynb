{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, AdamW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "autocast_questions = json.load(open('../competition/autocast_questions.json')) # from the Autocast dataset\n",
    "test_questions = json.load(open('../competition/autocast_competition_test_set.json'))\n",
    "test_ids = [q['id'] for q in test_questions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>id</th>\n",
       "      <th>background</th>\n",
       "      <th>publish_time</th>\n",
       "      <th>close_time</th>\n",
       "      <th>tags</th>\n",
       "      <th>source_links</th>\n",
       "      <th>prediction_count</th>\n",
       "      <th>forecaster_count</th>\n",
       "      <th>answer</th>\n",
       "      <th>choices</th>\n",
       "      <th>status</th>\n",
       "      <th>qtype</th>\n",
       "      <th>crowd</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What will the end-of-day closing value for the...</td>\n",
       "      <td>G1</td>\n",
       "      <td>Outcome will be determined by the end-of-day c...</td>\n",
       "      <td>2015-09-01 13:49:29.860000+00:00</td>\n",
       "      <td>2016-01-01 17:00:01+00:00</td>\n",
       "      <td>[Finance, Economic Indicators]</td>\n",
       "      <td>[http://ftalphaville.ft.com/2015/08/17/2137329...</td>\n",
       "      <td>1549</td>\n",
       "      <td>385</td>\n",
       "      <td>D</td>\n",
       "      <td>[Less than 6.30, Between 6.30 and 6.35, inclus...</td>\n",
       "      <td>Resolved</td>\n",
       "      <td>mc</td>\n",
       "      <td>[{'timestamp': '2015-09-01 00:00:00+00:00', 'f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>How many seats will the Justice and Developmen...</td>\n",
       "      <td>G2</td>\n",
       "      <td>The Justice and Development Party (AKP) failed...</td>\n",
       "      <td>2015-09-01 13:54:25.050000+00:00</td>\n",
       "      <td>2015-11-01 22:00:20+00:00</td>\n",
       "      <td>[Elections and Referenda, Non-US Politics]</td>\n",
       "      <td>[http://www.al-monitor.com/pulse/originals/201...</td>\n",
       "      <td>567</td>\n",
       "      <td>194</td>\n",
       "      <td>A</td>\n",
       "      <td>[A majority, A plurality, Not a plurality]</td>\n",
       "      <td>Resolved</td>\n",
       "      <td>mc</td>\n",
       "      <td>[{'timestamp': '2015-09-01 00:00:00+00:00', 'f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Will there be an initial public offering on ei...</td>\n",
       "      <td>G4</td>\n",
       "      <td>China suspended initial public offerings (IPOs...</td>\n",
       "      <td>2015-09-01 13:58:30.138000+00:00</td>\n",
       "      <td>2015-11-30 14:00:15+00:00</td>\n",
       "      <td>[Finance]</td>\n",
       "      <td>[http://atimes.com/2015/11/china-will-allow-su...</td>\n",
       "      <td>545</td>\n",
       "      <td>148</td>\n",
       "      <td>yes</td>\n",
       "      <td>[yes, no]</td>\n",
       "      <td>Resolved</td>\n",
       "      <td>t/f</td>\n",
       "      <td>[{'timestamp': '2015-09-01 00:00:00+00:00', 'f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Will the Export-Import Bank of the United Stat...</td>\n",
       "      <td>G5</td>\n",
       "      <td>The Export-Import Bank's authorization expired...</td>\n",
       "      <td>2015-09-01 14:02:21.242000+00:00</td>\n",
       "      <td>2015-12-04 14:00:25+00:00</td>\n",
       "      <td>[Economic Policy, US Politics, US Policy]</td>\n",
       "      <td>[http://thehill.com/policy/finance/260118-week...</td>\n",
       "      <td>1000</td>\n",
       "      <td>379</td>\n",
       "      <td>yes</td>\n",
       "      <td>[yes, no]</td>\n",
       "      <td>Resolved</td>\n",
       "      <td>t/f</td>\n",
       "      <td>[{'timestamp': '2015-09-01 00:00:00+00:00', 'f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Will a trilateral meeting take place between C...</td>\n",
       "      <td>G6</td>\n",
       "      <td>A trilateral meeting of leaders from China, Ja...</td>\n",
       "      <td>2015-09-01 14:04:41.470000+00:00</td>\n",
       "      <td>2015-12-31 23:00:11+00:00</td>\n",
       "      <td>[Foreign Policy]</td>\n",
       "      <td>[https://en.wikipedia.org/wiki/Li_Keqiang, htt...</td>\n",
       "      <td>946</td>\n",
       "      <td>385</td>\n",
       "      <td>no</td>\n",
       "      <td>[yes, no]</td>\n",
       "      <td>Resolved</td>\n",
       "      <td>t/f</td>\n",
       "      <td>[{'timestamp': '2015-09-01 00:00:00+00:00', 'f...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            question  id  \\\n",
       "0  What will the end-of-day closing value for the...  G1   \n",
       "1  How many seats will the Justice and Developmen...  G2   \n",
       "2  Will there be an initial public offering on ei...  G4   \n",
       "3  Will the Export-Import Bank of the United Stat...  G5   \n",
       "4  Will a trilateral meeting take place between C...  G6   \n",
       "\n",
       "                                          background  \\\n",
       "0  Outcome will be determined by the end-of-day c...   \n",
       "1  The Justice and Development Party (AKP) failed...   \n",
       "2  China suspended initial public offerings (IPOs...   \n",
       "3  The Export-Import Bank's authorization expired...   \n",
       "4  A trilateral meeting of leaders from China, Ja...   \n",
       "\n",
       "                       publish_time                 close_time  \\\n",
       "0  2015-09-01 13:49:29.860000+00:00  2016-01-01 17:00:01+00:00   \n",
       "1  2015-09-01 13:54:25.050000+00:00  2015-11-01 22:00:20+00:00   \n",
       "2  2015-09-01 13:58:30.138000+00:00  2015-11-30 14:00:15+00:00   \n",
       "3  2015-09-01 14:02:21.242000+00:00  2015-12-04 14:00:25+00:00   \n",
       "4  2015-09-01 14:04:41.470000+00:00  2015-12-31 23:00:11+00:00   \n",
       "\n",
       "                                         tags  \\\n",
       "0              [Finance, Economic Indicators]   \n",
       "1  [Elections and Referenda, Non-US Politics]   \n",
       "2                                   [Finance]   \n",
       "3   [Economic Policy, US Politics, US Policy]   \n",
       "4                            [Foreign Policy]   \n",
       "\n",
       "                                        source_links prediction_count  \\\n",
       "0  [http://ftalphaville.ft.com/2015/08/17/2137329...             1549   \n",
       "1  [http://www.al-monitor.com/pulse/originals/201...              567   \n",
       "2  [http://atimes.com/2015/11/china-will-allow-su...              545   \n",
       "3  [http://thehill.com/policy/finance/260118-week...             1000   \n",
       "4  [https://en.wikipedia.org/wiki/Li_Keqiang, htt...              946   \n",
       "\n",
       "  forecaster_count answer                                            choices  \\\n",
       "0              385      D  [Less than 6.30, Between 6.30 and 6.35, inclus...   \n",
       "1              194      A         [A majority, A plurality, Not a plurality]   \n",
       "2              148    yes                                          [yes, no]   \n",
       "3              379    yes                                          [yes, no]   \n",
       "4              385     no                                          [yes, no]   \n",
       "\n",
       "     status qtype                                              crowd  \n",
       "0  Resolved    mc  [{'timestamp': '2015-09-01 00:00:00+00:00', 'f...  \n",
       "1  Resolved    mc  [{'timestamp': '2015-09-01 00:00:00+00:00', 'f...  \n",
       "2  Resolved   t/f  [{'timestamp': '2015-09-01 00:00:00+00:00', 'f...  \n",
       "3  Resolved   t/f  [{'timestamp': '2015-09-01 00:00:00+00:00', 'f...  \n",
       "4  Resolved   t/f  [{'timestamp': '2015-09-01 00:00:00+00:00', 'f...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "autocast_questions_df = pd.DataFrame(autocast_questions)\n",
    "autocast_questions_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new dataframe with three columns\n",
    "mcq_df = pd.DataFrame(columns=[\"question\", \"choices\", \"answer\"])\n",
    "true_false_df = pd.DataFrame(columns=[\"question\", \"choices\", \"answer\"])\n",
    "\n",
    "# Iterate over the autocast_questions_df dataframe\n",
    "for index, row in autocast_questions_df.iterrows():\n",
    "    if row['id'] in test_ids: # skipping questions in the competition test set\n",
    "        continue\n",
    "    if row['answer'] is None: # skipping questions without answer\n",
    "        continue\n",
    "    if row['qtype'] == 'mc':\n",
    "    # Extract the question and choices from the row\n",
    "        question_mc = row[\"question\"]\n",
    "        choices_mc = row[\"choices\"]\n",
    "        answer_mc = row['answer']\n",
    "\n",
    "        # Add a new row to the new dataframe\n",
    "        new_row_mcq = {\"question\": question_mc, \"choices\": choices_mc, \"answer\": answer_mc}\n",
    "        mcq_df = pd.concat([mcq_df, pd.DataFrame([new_row_mcq])], ignore_index=True)\n",
    "    \n",
    "    # Extract the question and choices from the row\n",
    "    elif row['qtype'] == 't/f':\n",
    "        question_tf = row[\"question\"]\n",
    "        choices_tf = row[\"choices\"]\n",
    "        answer_tf = row['answer']\n",
    "\n",
    "        # Add a new row to the new dataframe\n",
    "        new_row_tf = {\"question\": question_tf, \"choices\": choices_tf, \"answer\": answer_tf}\n",
    "        true_false_df = pd.concat([true_false_df, pd.DataFrame([new_row_tf])], ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "mcq_df.head()\n",
    "mcq_df.to_csv('../competition/mcq-data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_false_df.head()\n",
    "true_false_df.to_csv('../competition/true-false.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"true-false.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()\n",
    "df.drop('Unnamed: 0', axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Will there be an initial public offering on either the Shanghai Stock Exchange or the Shenzhen Stock Exchange before 1 January 2016?',\n",
       "       'Will the Export-Import Bank of the United States be re-authorized before 1 January 2016?',\n",
       "       'Will a trilateral meeting take place between Chinese President Xi Jinping, Japanese Prime Minister Shinzo Abe, and South Korean President Park Geun-hye before 1 January 2016?',\n",
       "       ...,\n",
       "       'Will the US experience a 4th wave of COVID before June 1, 2021?',\n",
       "       'Will Brazil have a 7-day rolling average above 2,500 COVID-19 deaths before 1 June 2021?',\n",
       "       'Will Derek Chauvin be convicted of homicide by June 1 2021?'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " df.question.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-large-uncased')\n",
    "\n",
    "encoded_data = tokenizer.batch_encode_plus(\n",
    "    df['question'],\n",
    "    add_special_tokens=True,\n",
    "    return_attention_mask=True,\n",
    "    pad_to_max_length=True,\n",
    "    max_length=256,\n",
    "    return_tensors='pt',\n",
    "    truncation = True,\n",
    "    padding = 'max_length'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer = df.answer.values\n",
    "answer[answer == 'yes'] = 1\n",
    "answer[answer == 'no'] = 0\n",
    "labels = torch.tensor(answer.astype('int64'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1590])\n"
     ]
    }
   ],
   "source": [
    "print(labels.shape)\n",
    "# df['answer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 101, 2097, 2045,  ...,    0,    0,    0],\n",
       "        [ 101, 2097, 1996,  ...,    0,    0,    0],\n",
       "        [ 101, 2097, 1037,  ...,    0,    0,    0],\n",
       "        ...,\n",
       "        [ 101, 2097, 1996,  ...,    0,    0,    0],\n",
       "        [ 101, 2097, 4380,  ...,    0,    0,    0],\n",
       "        [ 101, 2097, 7256,  ...,    0,    0,    0]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_data['input_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = encoded_data['input_ids']\n",
    "attention_masks = encoded_data['attention_mask']\n",
    "\n",
    "dataset = torch.utils.data.TensorDataset(input_ids, attention_masks, labels)\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-large-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-large-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/Users/praneshjayasundar/Library/Python/3.9/lib/python/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    \"bert-large-uncased\",\n",
    "    num_labels=2,\n",
    "    output_attentions=False,\n",
    "    output_hidden_states=False\n",
    ")\n",
    "\n",
    "batch_size = 32\n",
    "epochs = 5\n",
    "learning_rate = 2e-5\n",
    "optimizer = AdamW(model.parameters(), lr=learning_rate, eps=1e-8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = torch.utils.data.DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "val_dataloader = torch.utils.data.DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.62\n",
      "Validation Accuracy: 0.74\n",
      "Training loss: 0.59\n",
      "Validation Accuracy: 0.74\n",
      "Training loss: 0.56\n",
      "Validation Accuracy: 0.74\n",
      "Training loss: 0.48\n",
      "Validation Accuracy: 0.69\n",
      "Training loss: 0.37\n",
      "Validation Accuracy: 0.61\n"
     ]
    }
   ],
   "source": [
    "# Step 8: Define the device and move model and data to the device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "for epoch in range(1, epochs+1):\n",
    "    model.train()\n",
    "\n",
    "    total_loss = 0\n",
    "    for batch in train_dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        input_ids = batch[0].to(device)\n",
    "        attention_masks = batch[1].to(device)\n",
    "        labels = batch[2].to(device)\n",
    "        outputs = model(input_ids, attention_mask=attention_masks, labels=labels)\n",
    "        loss = outputs[0]\n",
    "        total_loss += loss.item()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "\n",
    "    avg_train_loss = total_loss / len(train_dataloader)\n",
    "    print(\"Training loss: {0:.2f}\".format(avg_train_loss))\n",
    "\n",
    "    model.eval()\n",
    "    val_accuracy = 0\n",
    "    for batch in val_dataloader:\n",
    "        input_ids = batch[0].to(device)\n",
    "        attention_masks = batch[1].to(device)\n",
    "        labels = batch[2].to(device)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_ids, attention_mask=attention_masks)\n",
    "        logits = outputs[0]\n",
    "        preds = torch.argmax(logits, dim=1).flatten()\n",
    "        accuracy = torch.sum(preds == labels).item()\n",
    "        val_accuracy += accuracy\n",
    "    avg_val_accuracy = val_accuracy / len(val_dataset)\n",
    "    print(\"Validation Accuracy: {0:.2f}\".format(avg_val_accuracy))\n",
    "\n",
    "    torch.save(model.state_dict(), 'model.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_baseline_model(question):\n",
    "    if question['qtype'] == 't/f':\n",
    "        return np.random.random(size=2)\n",
    "    elif question['qtype'] == 'mc':\n",
    "        probs = np.random.random(size=len(question['choices']))\n",
    "        return probs / probs.sum()\n",
    "    elif question['qtype'] == 'num':\n",
    "        return np.random.random()\n",
    "\n",
    "def caliberated_tf_pred(question):\n",
    "\n",
    "    # Define the question and choices\n",
    "    actual_question = question['question']\n",
    "    tags = question['tags']\n",
    "    choice1 = question['choices'][0]\n",
    "    choice2 = question['choices'][1]\n",
    "\n",
    "    # Combine the question and choices into a single input string\n",
    "    input_text = [f\"{actual_question} {choice1} {choice2}\"]\n",
    "    \n",
    "    # Tokenize the input string and convert to tensors\n",
    "    inputs = tokenizer(input_text, return_tensors='pt', padding=True, truncation=True, max_length= 512)\n",
    "\n",
    "    # Make a prediction with the model\n",
    "    outputs = model(**inputs)\n",
    "    probs = torch.softmax(outputs.logits, dim=1)\n",
    "\n",
    "    return probs.detach().flatten().numpy()\n",
    "\n",
    "# def caliberated_mcq_pred(question):\n",
    "\n",
    "#     # Define the question and choices\n",
    "#     actual_question = question['question']\n",
    "#     choices = question['choices']\n",
    "\n",
    "#     # Combine the question and choices into a single input string\n",
    "#     input_text = [f\"{actual_question} {choice}\" for choice in choices]\n",
    "#     # print(input_text)\n",
    "    \n",
    "#     # Tokenize the input string and convert to tensors\n",
    "#     inputs = mcq_tokenizer(input_text, return_tensors='pt', padding=True, truncation=True)\n",
    "\n",
    "#     # Make a prediction with the model\n",
    "#     outputs = mcq_model(**inputs)\n",
    "#     probs = torch.softmax(outputs.logits, dim=1)\n",
    "    \n",
    "#     choice_probs = probs[:, 1].detach().flatten().numpy()\n",
    "\n",
    "#     return choice_probs\n",
    "\n",
    "# def caliberated_mcq_pred(question):\n",
    "\n",
    "#     # Define the question and choices\n",
    "#     actual_question = question['question']\n",
    "#     choices = question['choices']\n",
    "\n",
    "#     # Combine the question and choices into a single input string\n",
    "#     input_text = [actual_question] + choices\n",
    "#     # print(input_text)\n",
    "    \n",
    "#     # Tokenize the input string and convert to tensors\n",
    "#     inputs = mcq_tokenizer(input_text, return_tensors='pt', padding=True, truncation=True)\n",
    "\n",
    "#     # Make a prediction with the model\n",
    "#     outputs = mcq_model(**inputs)\n",
    "#     probs = torch.softmax(outputs.logits, dim=1)\n",
    "\n",
    "#     return probs.detach().flatten().numpy()[:len(choices)]\n",
    "\n",
    "\n",
    "def calibrated_random_baseline_model(question):\n",
    "    if question['qtype'] == 't/f':\n",
    "        # pred_idx = np.argmax(np.random.random(size=2))\n",
    "        # pred = np.ones(2)\n",
    "        # pred[pred_idx] += 1e-3\n",
    "        # return pred / pred.sum()\n",
    "        return caliberated_tf_pred(question)\n",
    "    elif question['qtype'] == 'mc':\n",
    "        pred_idx = np.argmax(np.random.random(size=len(question['choices'])))\n",
    "        pred = np.ones(len(question['choices']))\n",
    "        pred[pred_idx] += 1e-3\n",
    "        return pred / pred.sum()\n",
    "        # return caliberated_mcq_pred(question)\n",
    "    elif question['qtype'] == 'num':\n",
    "        #return 0.40\n",
    "        return np.random.uniform(0.38, 0.43)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def brier_score(probabilities, answer_probabilities):\n",
    "    return ((probabilities - answer_probabilities) ** 2).sum() / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = []\n",
    "answers = []\n",
    "qtypes = []\n",
    "for question in autocast_questions:\n",
    "    if question['id'] in test_ids: # skipping questions in the competition test set\n",
    "        continue\n",
    "    if question['answer'] is None: # skipping questions without answer\n",
    "        continue\n",
    "    preds.append(calibrated_random_baseline_model(question))\n",
    "    if question['qtype'] == 't/f':\n",
    "        ans_idx = 0 if question['answer'] == 'no' else 1\n",
    "        ans = np.zeros(len(question['choices']))\n",
    "        ans[ans_idx] = 1\n",
    "        qtypes.append('t/f')\n",
    "    elif question['qtype'] == 'mc':\n",
    "        ans_idx = ord(question['answer']) - ord('A')\n",
    "        ans = np.zeros(len(question['choices']))\n",
    "        ans[ans_idx] = 1\n",
    "        qtypes.append('mc')\n",
    "    elif question['qtype'] == 'num':\n",
    "        ans = float(question['answer'])\n",
    "        qtypes.append('num')\n",
    "    answers.append(ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T/F: 19.36, MCQ: 38.05, NUM: 21.18\n",
      "Combined Metric: 78.58\n"
     ]
    }
   ],
   "source": [
    "tf_results, mc_results, num_results = [],[],[]\n",
    "\n",
    "for p, a, qtype in zip(preds, answers, qtypes):\n",
    "    if qtype == 't/f':\n",
    "        tf_results.append(brier_score(p, a))\n",
    "    elif qtype == 'mc':\n",
    "        mc_results.append(brier_score(p, a))\n",
    "    else:\n",
    "        num_results.append(np.abs(p - a))\n",
    "\n",
    "print(f\"T/F: {np.mean(tf_results)*100:.2f}, MCQ: {np.mean(mc_results)*100:.2f}, NUM: {np.mean(num_results)*100:.2f}\")\n",
    "print(f\"Combined Metric: {(np.mean(tf_results) + np.mean(mc_results) + np.mean(num_results))*100:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = []\n",
    "for question in test_questions:\n",
    "    preds.append(calibrated_random_baseline_model(question))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "updating: predictions.pkl (deflated 64%)\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists('submission'):\n",
    "    os.makedirs('submission')\n",
    "\n",
    "with open(os.path.join('submission', 'predictions.pkl'), 'wb') as f:\n",
    "    pickle.dump(preds, f, protocol=2)\n",
    "\n",
    "!cd submission && zip ../submission.zip ./* && cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
